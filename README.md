# EnvEval - Environment Evaluation Benchmark

ğŸŒŸ **A Comprehensive Benchmark for Environment Configuration and Code Execution**

EnvEval is a benchmark suite specifically designed to evaluate AI models' performance on environment configuration and code execution tasks.

## ğŸ“ Project Structure

```
EnvEval/
â”œâ”€â”€ README.md           # Project documentation
â”œâ”€â”€ LICENSE            # Open source license
â”œâ”€â”€ website/           # Benchmark website
â”‚   â”œâ”€â”€ index.html    # Main page
â”‚   â”œâ”€â”€ styles.css    # Stylesheets
â”‚   â””â”€â”€ script.js     # Interactive scripts
â”œâ”€â”€ benchmark/         # New benchmark directory with additional tasks and scripts
â””â”€â”€ dataset/           # Dataset and results
    â”œâ”€â”€ index.json    # Benchmark data index
    â””â”€â”€ results/      # Result folders (one per model)
```

## ğŸ“Š Benchmark Results

### Baseline Methods

- **Codex**: GPT-4.1 (26.15%), GPT-4.1-mini (22.31%)
- **Claude-Code**: Claude-Opus-4 (30.10%), Claude-Haiku-3.5 (18.21%)

### EnvGym Method (Ours)

- **Claude-Opus-4**: 74.01% ğŸ† (Highest Score)
- **GPT-4.1**: 70.07%
- **Gemini-2.5-Pro**: 66.81%
- **GPT-4.1-mini**: 53.41%
- **Claude-Haiku-3.5**: 50.59%
- **DeepSeek-V3**: 49.54%
- **DeepSeek-R1**: 42.25%

## ğŸš€ Quick Start

### View Website

```bash
# Recommended: Use the development server (supports data loading)
cd website
python start_server.py
# Visit http://localhost:8000/website/

# Alternative: Simple server from project root
python -m http.server 8000
# Visit http://localhost:8000/website/
```

### âœ¨ Dynamic Data Loading

The website automatically loads results from the `dataset/` folder:

- **Real-time updates**: Add new results by updating dataset files
- **No hardcoding**: All benchmark data loaded dynamically from JSON
- **Interactive**: Click any result row for detailed information

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ”— Links

- **ğŸŒ Website**: [Online Demo URL]
- **ğŸ“¦ GitHub**: [GitHub Repository URL]
- **ğŸ“„ Paper**: [arXiv Link]

---

**EnvEval Team**
